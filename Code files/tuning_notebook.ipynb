{"cells":[{"cell_type":"code","metadata":{"source_hash":"96d117af","execution_start":1734366696353,"execution_millis":3685,"execution_context_id":"e7e3c053-c9c8-4c12-ac24-9878d7e18268","cell_id":"228879e56d324bf187d72ad2f826f189","deepnote_cell_type":"code"},"source":"!pip install ioh","block_group":"e55cb3a0e4bb46a6874e80fbda02ac09","execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ioh\n  Downloading ioh-0.3.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from ioh) (1.23.4)\nInstalling collected packages: ioh\nSuccessfully installed ioh-0.3.18\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ed09c94a-7e4f-4d71-96d8-5c9eec3631c3","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"c264d352","execution_start":1734366700085,"execution_millis":231,"execution_context_id":"e7e3c053-c9c8-4c12-ac24-9878d7e18268","cell_id":"bfb80f423ade45e79415dfe09ae790b7","deepnote_cell_type":"code"},"source":"from typing import List\n\nimport numpy as np\n# you need to install this package `ioh`. Please see documentations here: \n# https://iohprofiler.github.io/IOHexp/ and https://pypi.org/project/ioh/\nfrom ioh import get_problem, logger, ProblemClass\n# import GA\nfrom GA import s4018907_s4168216_GA, create_problem","block_group":"c2a4d78598c4432a9e4c6b1371c2ba93","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"bb55d451","execution_start":1734366700405,"execution_millis":0,"execution_context_id":"e7e3c053-c9c8-4c12-ac24-9878d7e18268","cell_id":"62376443af8a4a6c8036929fe84064fb","deepnote_cell_type":"code"},"source":"from typing import Tuple \nimport numpy as np\nimport ioh\nfrom ioh import get_problem, logger, ProblemClass\n\ndef n_crossover(p1, p2, size, n=2, crossover_rate = 0.5):\n\n    if np.random.rand() > crossover_rate:\n        # If not, just return the parents as children (no crossover)\n        return p1, p2\n\n    split_positions = sorted(np.random.choice(range(size), n, replace=False))\n    c1 = []\n    c2 = []\n    subarrays_p1 = np.split(p1, split_positions)\n    subarrays_p2 = np.split(p2, split_positions)\n    \n    for i in range(len(subarrays_p1)):\n        if (i % 2 == 0):\n            for j in range(len(subarrays_p1[i])):\n                c1.append(subarrays_p1[i][j])\n                c2.append(subarrays_p2[i][j])\n        else: \n            for j in range(len(subarrays_p2[i])):\n                c1.append(subarrays_p2[i][j])\n                c2.append(subarrays_p1[i][j])\n    return c1,c2\n\ndef mating_selection(population, pop_fitness):\n    f_min = min(pop_fitness)\n    f_sum = sum(pop_fitness) - (f_min - 0.001) * len(pop_fitness)\n    \n    rw = [(pop_fitness[0] - f_min + 0.001)/f_sum]\n    for i in range(1,len(pop_fitness)):\n        rw.append(rw[i-1] + (pop_fitness[i] - f_min + 0.001) / f_sum)\n    \n    select_parent = []\n    for i in range(2) :\n        r = np.random.uniform(0,1)\n        index = 0\n        # print(rw,r)\n        while(r > rw[index]) :\n            index = index + 1\n        \n        select_parent.append(population[index].copy())\n    return select_parent\n\ndef mutate(c, mutation_rate):\n    ind_length = len(c)\n    for j in range(ind_length):  \n        if np.random.uniform(0, 1) < mutation_rate:\n            swap_idx = np.random.randint(0, ind_length)\n            c[j], c[swap_idx] = c[swap_idx], c[j]  # Swap mutation\n\n    return c\n\ndef s4018907_s4168216_GA(problem: ioh.problem.PBO, init_pop_size: int, mutation_rate: float, crossover_rate: float, budget: int) -> None:\n    # initial_pop = ... make sure you randomly create the first population\n    #initial_pop_size = pop_size\n    # mutation_rate = \n    #crossover_rate = 0.5\n    population = []\n    pop_fitness = []\n\n    for i in range(init_pop_size):\n        # Initialization\n        population.append(np.random.randint(2, size = problem.meta_data.n_variables))\n        pop_fitness.append(problem(population[i]))\n\n    # `problem.state.evaluations` counts the number of function evaluation automatically,\n    # which is incremented by 1 whenever you call `problem(x)`.\n    # You could also maintain a counter of function evaluations if you prefer.\n    while problem.state.evaluations < budget:\n        parents = mating_selection(population, pop_fitness)\n        p1 = parents[0]\n        p2 = parents[1]\n        c1, c2 = n_crossover(p1, p2, problem.meta_data.n_variables, crossover_rate = crossover_rate)\n        mutated_c1 = mutate(c1, mutation_rate)\n        mutated_c2 = mutate(c2, mutation_rate)\n        f1 = problem(mutated_c1)\n        f2 = problem(mutated_c2)\n        #print(\"f1\", f1)\n        #print(\"f2\", f2)\n        population.append(mutated_c1)\n        population.append(mutated_c2)\n    print(\"F1: \", f1)\n    print(\"F2: \", f2)\n    return f1, f2 \n\n\ndef create_problem(dimension: int, fid: int, name:str) -> Tuple[ioh.problem.PBO, ioh.logger.Analyzer]:\n    # Declaration of problems to be tested.\n    problem = get_problem(fid, dimension=dimension, instance=1, problem_class=ProblemClass.PBO)\n    print(\"creating problem\")\n    # Create default logger compatible with IOHanalyzer\n    # `root` indicates where the output files are stored.\n    # `folder_name` is the name of the folder containing all output. You should compress the folder 'run' and upload it to IOHanalyzer.\n    l = logger.Analyzer(\n        root=\"data\",  # the working directory in which a folder named `folder_name` (the next argument) will be created to store data\n        folder_name=\"run\",  # the folder name to which the raw performance data will be stored\n        algorithm_name= name,  # name of your algorithm\n        algorithm_info=\"Practical assignment of the EA course\",\n    )\n    # attach the logger to the problem\n    problem.attach_logger(l)\n    return problem, l\n\n","block_group":"cfc7da0c4de34a599c5dc7a2a7d9e857","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"f452358","execution_start":1734366700453,"execution_millis":0,"execution_context_id":"e7e3c053-c9c8-4c12-ac24-9878d7e18268","cell_id":"3e4a31749cce4b2b8514c60e39d3b2c1","deepnote_cell_type":"code"},"source":"budget = 5000000\n\n# To make your results reproducible (not required by the assignment), you could set the random seed by\n# `np.random.seed(some integer, e.g., 42)`\n\n# Hyperparameters to tune, e.g.\nnp.random.seed(42)\nhyperparameter_space = {\n    \"population_size\": [50, 100, 200, 500, 1000],\n    \"mutation_rate\": [0.01, 0.05, 0.1],\n    \"crossover_rate\": [0.6, 0.8]\n}\n","block_group":"d0a85c9ba2ec4705884f9496705f165b","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9025c46e","execution_start":1734366715073,"execution_millis":0,"execution_context_id":"e7e3c053-c9c8-4c12-ac24-9878d7e18268","cell_id":"ef7dbfdecac049f88e8ed480aad4aa48","deepnote_cell_type":"code"},"source":"# Fixed Hyperparameter tuning function with the positional argument issue resolved\ndef tune_hyperparameters() -> List:\n    # You should decide/engineer the `score` yourself, which is the tuning objective\n    best_score = float('-inf')\n    best_params = None\n    # create the LABS problem and the data logger\n\n\n    for pop_size in hyperparameter_space['population_size']:\n        for mutation_rate in hyperparameter_space['mutation_rate']:\n            for crossover_rate in hyperparameter_space['crossover_rate']:\n\n                F18, _logger18 = create_problem(dimension=50, fid=18, name=f\"pop_size={pop_size}_mr={mutation_rate}_cr={crossover_rate}\")\n                # create the N-Queens problem and the data logge\n                F23, _logger23 = create_problem(dimension=49, fid=23, name=f\"pop_size={pop_size}_mr={mutation_rate}_cr={crossover_rate}\")\n\n                \n                print(\"running hyperparameter tuning for \", pop_size, mutation_rate, crossover_rate)\n\n                print(\"Running GA on F18\")\n                score_f18_1, score_f18_2 = s4018907_s4168216_GA(F18, pop_size, mutation_rate, crossover_rate, budget)\n                score_f18 = np.mean([score_f18_1, score_f18_2])\n                print(\"score for F18: \", score_f18)\n\n                F18.reset()\n\n                if score_f18 > best_score:\n                    best_score = score_f18\n                    print(\"best score for F18: \", best_score)\n                    best_params = [pop_size, mutation_rate, crossover_rate]\n\n                print(\"Running GA on F23\")\n                score_f23_1, score_f23_2 = s4018907_s4168216_GA(F23, pop_size, mutation_rate, crossover_rate, budget)\n                score_f23 = np.mean([score_f23_1, score_f23_2])\n                print(\"score for F23: \", score_f23)\n\n                F23.reset()\n\n                if score_f23 > best_score:\n                    best_score = score_f23\n                    print(\"best score for F23: \", best_score)\n                    best_params = [pop_size, mutation_rate, crossover_rate]\n\n    return best_params","block_group":"ed1322f008ce4b27bd71f3d5923ea3b1","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"f7a2605c","execution_start":1734366717111,"execution_millis":1,"execution_context_id":"f8991b16-0526-4251-8192-6287b034b130","cell_id":"7cba5a010b14412f820f8e2d56937167","deepnote_cell_type":"code"},"source":"    '''\n        for mutation_rate in hyperparameter_space['mutation_rate']:\n        for crossover_rate in hyperparameter_space['crossover_rate']:\n    '''","block_group":"ce2b580a4bd645a58c3e29d9a2870f02","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"\"\\n    for mutation_rate in hyperparameter_space['mutation_rate']:\\n    for crossover_rate in hyperparameter_space['crossover_rate']:\\n\""},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/c942c4af-f709-4222-9be4-634342109d7c","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"afbda785","execution_start":1734366754257,"execution_millis":47507716,"execution_context_id":"e7e3c053-c9c8-4c12-ac24-9878d7e18268","cell_id":"b287c7473d524b8e9c8da57ca74cb369","deepnote_cell_type":"code"},"source":"# The error occurs because the function 'tune_hyperparameters' is returning a single object (likely a float or another value),\n# while the code expects it to return three unpackable values (population_size, mutation_rate, crossover_rate).\n\n# Fix: Ensure that the 'tune_hyperparameters' function returns a tuple with 3 values or change the calling code to match\n# the actual return value of the function.\n\nif __name__ == \"__main__\":\n\n    # Call the hyperparameter tuning function\n    best_params = tune_hyperparameters()\n    #if len(best_params) == 3:\n    population_size, mutation_rate, crossover_rate = best_params\n    print(population_size)\n    print(mutation_rate)\n    print(crossover_rate)\n","block_group":"6ec64fe82be04fe099128545470f454f","execution_count":8,"outputs":[{"name":"stdout","text":"creating problem\ncreating problem\nrunning hyperparameter tuning for  50 0.01 0.6\nRunning GA on F18\nF1:  0.8350033400133601\nF2:  1.6960651289009498\nscore for F18:  1.265534234457155\nbest score for F18:  1.265534234457155\nRunning GA on F23\nF1:  -457.0\nF2:  -254.0\nscore for F23:  -355.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  50 0.01 0.8\nRunning GA on F18\nF1:  1.0548523206751055\nF2:  0.9881422924901185\nscore for F18:  1.021497306582612\nRunning GA on F23\nF1:  -336.0\nF2:  -356.0\nscore for F23:  -346.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  50 0.05 0.6\nRunning GA on F18\nF1:  2.1079258010118043\nF2:  1.2388503468780971\nscore for F18:  1.6733880739449507\nbest score for F18:  1.6733880739449507\nRunning GA on F23\nF1:  -423.0\nF2:  -389.0\nscore for F23:  -406.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  50 0.05 0.8\nRunning GA on F18\nF1:  1.4863258026159334\nF2:  1.2388503468780971\nscore for F18:  1.3625880747470154\nRunning GA on F23\nF1:  -416.0\nF2:  -510.0\nscore for F23:  -463.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  50 0.1 0.6\nRunning GA on F18\nF1:  0.9462528387585163\nF2:  1.2487512487512487\nscore for F18:  1.0975020437548824\nRunning GA on F23\nF1:  -295.0\nF2:  -309.0\nscore for F23:  -302.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  50 0.1 0.8\nRunning GA on F18\nF1:  1.2291052114060963\nF2:  1.0993843447669305\nscore for F18:  1.1642447780865135\nRunning GA on F23\nF1:  -396.0\nF2:  -363.0\nscore for F23:  -379.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  100 0.01 0.6\nRunning GA on F18\nF1:  1.1563367252543941\nF2:  1.3116474291710387\nscore for F18:  1.2339920772127164\nRunning GA on F23\nF1:  -349.0\nF2:  -322.0\nscore for F23:  -335.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  100 0.01 0.8\nRunning GA on F18\nF1:  1.0841283607979184\nF2:  0.8132726089785296\nscore for F18:  0.948700484888224\nRunning GA on F23\nF1:  -531.0\nF2:  -275.0\nscore for F23:  -403.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  100 0.05 0.6\nRunning GA on F18\nF1:  1.0548523206751055\nF2:  0.8486082824168364\nscore for F18:  0.9517303015459709\nRunning GA on F23\nF1:  -369.0\nF2:  -430.0\nscore for F23:  -399.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  100 0.05 0.8\nRunning GA on F18\nF1:  1.2899896800825594\nF2:  1.5006002400960383\nscore for F18:  1.395294960089299\nRunning GA on F23\nF1:  -309.0\nF2:  -396.0\nscore for F23:  -352.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  100 0.1 0.6\nRunning GA on F18\nF1:  1.1825922421948911\nF2:  0.6222000995520159\nscore for F18:  0.9023961708734536\nRunning GA on F23\nF1:  -375.0\nF2:  -342.0\nscore for F23:  -358.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  100 0.1 0.8\nRunning GA on F18\nF1:  1.2007684918347743\nF2:  1.3691128148959475\nscore for F18:  1.284940653365361\nRunning GA on F23\nF1:  -396.0\nF2:  -484.0\nscore for F23:  -440.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  200 0.01 0.6\nRunning GA on F18\nF1:  0.952018278750952\nF2:  0.7654623392529087\nscore for F18:  0.8587403090019303\nRunning GA on F23\nF1:  -322.0\nF2:  -356.0\nscore for F23:  -339.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  200 0.01 0.8\nRunning GA on F18\nF1:  0.821827744904668\nF2:  1.1071744906997343\nscore for F18:  0.9645011178022012\nRunning GA on F23\nF1:  -349.0\nF2:  -335.0\nscore for F23:  -342.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  200 0.05 0.6\nRunning GA on F18\nF1:  1.0072522159548751\nF2:  0.9462528387585163\nscore for F18:  0.9767525273566957\nRunning GA on F23\nF1:  -309.0\nF2:  -322.0\nscore for F23:  -315.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  200 0.05 0.8\nRunning GA on F18\nF1:  1.1071744906997343\nF2:  0.9184423218221895\nscore for F18:  1.012808406260962\nRunning GA on F23\nF1:  -396.0\nF2:  -410.0\nscore for F23:  -403.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  200 0.1 0.6\nRunning GA on F18\nF1:  1.3116474291710387\nF2:  0.7034327518289252\nscore for F18:  1.0075400904999818\nRunning GA on F23\nF1:  -295.0\nF2:  -470.0\nscore for F23:  -382.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  200 0.1 0.8\nRunning GA on F18\nF1:  1.2588116817724069\nF2:  1.3812154696132597\nscore for F18:  1.3200135756928333\nRunning GA on F23\nF1:  -248.0\nF2:  -491.0\nscore for F23:  -369.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  500 0.01 0.6\nRunning GA on F18\nF1:  0.7617306520414382\nF2:  1.3007284079084287\nscore for F18:  1.0312295299749334\nRunning GA on F23\nF1:  -369.0\nF2:  -490.0\nscore for F23:  -429.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  500 0.01 0.8\nRunning GA on F18\nF1:  1.5605493133583022\nF2:  1.9747235387045814\nscore for F18:  1.7676364260314417\nbest score for F18:  1.7676364260314417\nRunning GA on F23\nF1:  -436.0\nF2:  -282.0\nscore for F23:  -359.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  500 0.05 0.6\nRunning GA on F18\nF1:  0.9077705156136529\nF2:  0.952018278750952\nscore for F18:  0.9298943971823024\nRunning GA on F23\nF1:  -342.0\nF2:  -396.0\nscore for F23:  -369.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  500 0.05 0.8\nRunning GA on F18\nF1:  1.3455328310010763\nF2:  1.147842056932966\nscore for F18:  1.246687443967021\nRunning GA on F23\nF1:  -517.0\nF2:  -349.0\nscore for F23:  -433.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  500 0.1 0.6\nRunning GA on F18\nF1:  1.5923566878980893\nF2:  1.5762925598991173\nscore for F18:  1.5843246238986033\nRunning GA on F23\nF1:  -369.0\nF2:  -335.0\nscore for F23:  -352.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  500 0.1 0.8\nRunning GA on F18\nF1:  1.1230907457322552\nF2:  0.7926442612555485\nscore for F18:  0.9578675034939019\nRunning GA on F23\nF1:  -517.0\nF2:  -362.0\nscore for F23:  -439.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  1000 0.01 0.6\nRunning GA on F18\nF1:  0.994431185361973\nF2:  1.1916110581506196\nscore for F18:  1.0930211217562964\nRunning GA on F23\nF1:  -390.0\nF2:  -282.0\nscore for F23:  -336.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  1000 0.01 0.8\nRunning GA on F18\nF1:  1.0271158586688578\nF2:  0.862663906142167\nscore for F18:  0.9448898824055124\nRunning GA on F23\nF1:  -254.0\nF2:  -578.0\nscore for F23:  -416.0\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  1000 0.05 0.6\nRunning GA on F18\nF1:  1.2195121951219512\nF2:  1.2588116817724069\nscore for F18:  1.239161938447179\nRunning GA on F23\nF1:  -342.0\nF2:  -235.0\nscore for F23:  -288.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  1000 0.05 0.8\nRunning GA on F18\nF1:  0.5805852299117511\nF2:  1.1737089201877935\nscore for F18:  0.8771470750497723\nRunning GA on F23\nF1:  -652.0\nF2:  -403.0\nscore for F23:  -527.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  1000 0.1 0.6\nRunning GA on F18\nF1:  1.3691128148959475\nF2:  0.7768800497203232\nscore for F18:  1.0729964323081354\nRunning GA on F23\nF1:  -201.0\nF2:  -316.0\nscore for F23:  -258.5\ncreating problem\ncreating problem\nrunning hyperparameter tuning for  1000 0.1 0.8\nRunning GA on F18\nF1:  1.0993843447669305\nF2:  1.625487646293888\nscore for F18:  1.3624359955304093\nRunning GA on F23\nF1:  -309.0\nF2:  -416.0\nscore for F23:  -362.5\n500\n0.01\n0.8\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6db5e265-6708-4797-872d-2ba7f4738062","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b4bf3a4f","execution_start":1734195897322,"execution_millis":1,"execution_context_id":"df89815a-76c6-4b30-b3ae-b5527525ea44","cell_id":"f06d1c1f0a2e41249cbf86c2d579cba9","deepnote_cell_type":"code"},"source":"print(best_params)","block_group":"c35079b53cce4a6db44b12434012ebf9","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'best_params' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbest_params\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"]}],"outputs_reference":"dbtable:cell_outputs/f655d601-0793-4b36-90da-91485424d270","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"304ba53feee74a3885eec07dcb78df88","deepnote_cell_type":"code"},"source":"","block_group":"6f252f44d2db4055802d189a3e6ed058","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6809d180-5347-4c49-92ff-04f56374e52e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-12-17T05:45:13.664Z"},"deepnote_full_width":true,"deepnote_notebook_id":"631681163e424b0d89179c5aa105716a"}}